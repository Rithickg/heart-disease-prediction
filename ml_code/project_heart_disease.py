# -*- coding: utf-8 -*-
"""project_heart_disease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dJkDMg-G3L_5HcvEu0guU_tBNSyKUE9K
"""

# Data analysis
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Model selection
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

# Model valuation
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.metrics import precision_score,recall_score,f1_score
!pip install plot_roc_curve
from sklearn.metrics import plot_roc_curve

# Load Data
df =pd.read_csv("heart_disease.csv")
df.shape

df.head()

df.tail()

df.target.value_counts()

df["target"].value_counts().plot(kind="bar",color=["red","lightblue"])

df.info()

df.isna().sum()

df.describe()

df.sex.value_counts()

# compare target colum with sex column
pd.crosstab(df.target,df.sex)

# create a plot for crosstab
pd.crosstab(df.target,df.sex).plot(kind="bar",figsize=(10,6),color=["red","lightblue"])
plt.title("Heart Disease based on sex")
plt.xlabel("0 = No Disease ,1 = Disease")
plt.ylabel("Amount")
plt.legend(["Female","Male"])
plt.xticks(rotation=0);

# Age vs max heart rate (thalach)
plt.figure(figsize=(10,6))
# scatter with positive
plt.scatter(df.age[df.target==1],df.thalach[df.target==1],c="red")
# scatter with negative
plt.scatter(df.age[df.target==0],df.thalach[df.target==0],c="lightblue")
plt.title("Heart disease based on age and max heart rate")
plt.xlabel("Age")
plt.ylabel("Max Heart Rate")
plt.legend(["Disease","No Disease"])

# check the distribution or spread of data with histogram
df.age.plot.hist()

# heart disease frequency based on chest pain
pd.crosstab(df.cp,df.target)

pd.crosstab(df.cp,df.target).plot(kind="bar",figsize=(10,6),color=["red","lightblue"])
plt.title("heart disease frequency based on chest pain")
plt.xlabel("Chest pain type")
plt.ylabel("Amount")
plt.legend(["No Disease","Disease"])
plt.xticks(rotation=0);

# make a correlation matrix
df.corr()

# correlation matrix heatmap
corr_matrix=df.corr()
fig,ax =plt.subplots(figsize=(15,10))
ax=sns.heatmap(corr_matrix,annot=True,linewidths=0.5,fmt=".2f",cmap="YlGnBu")

x=df.drop("target",axis=1)
x

y=df["target"]
y

# Model in a dictionary
models ={"Logistic Regression": LogisticRegression(),
         "KNN": KNeighborsClassifier(),
         "Random Forest": RandomForestClassifier()}

# train the data
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)

# Create a function and fit and score the model
def fit_and_score(models,x_train,x_test,y_train,y_test):
  # set random seed
  np.random.seed(45)
  # empty dictionary to keep model score
  model_scores={}
  # loop through the models
  for name,model in models.items():
    # fit the model
    model.fit(x_train,y_train)
    # score the model and append to model_scores
    model_scores[name]=model.score(x_test,y_test)
  return model_scores

model_scores =fit_and_score(models=models,
                            x_train=x_train,
                            x_test=x_test,
                            y_train=y_train,
                            y_test=y_test)
model_scores

# comparing the different models
model_compare=pd.DataFrame(model_scores,index=["accuracy"])
model_compare.T.plot.bar();

# Tuning KNeighbors Classifier
# hyperparameter for KNN -n_neighbors(By hand)

train_scores =[]
test_scores=[]

neighbors=range(1,21)

knn=KNeighborsClassifier()

# loop through n_neighbors

for i in neighbors:
  knn.set_params(n_neighbors=i)
  # fit the algorithm
  knn.fit(x_train,y_train)
  # update train score
  train_scores.append(knn.score(x_train,y_train))
  # update test score
  test_scores.append(knn.score(x_test,y_test))

train_scores

test_scores

plt.plot(neighbors,train_scores,label="train Score")
plt.plot(neighbors,test_scores,label="test score")
plt.xticks(np.arange(1,21,1))
plt.xlabel("Number of neighbors")
plt.ylabel("Model score")
plt.legend()
print(f"Maximum KNN score on the test data:{max(test_scores)*100:.2f}%")

# Hyperparameter tuning with RandomizedSearchCV
# For Logistic Regression
log_reg_grid = {"C":np.logspace(-4,4,20),
                "solver":["liblinear"]}

# For RandomForestClassifier
rf_grid={"n_estimators":np.arange(10,1000,50),
         "max_depth":[None,3,5,10],
         "min_samples_split":np.arange(2,20,2),
         "min_samples_leaf":np.arange(1,20,2)}

# For Logistic Regression
np.random.seed(43)

rs_log_reg=RandomizedSearchCV(LogisticRegression(),
                              param_distributions=log_reg_grid,
                              cv=5,
                              n_iter=20,
                              verbose=True)
rs_log_reg.fit(x_train,y_train)

rs_log_reg.best_params_

rs_log_reg.score(x_test,y_test)

# For RandomForestClassifier
np.random.seed(43)
rs_rf=RandomizedSearchCV(RandomForestClassifier(),
                         param_distributions=rf_grid,
                         cv=5,
                         n_iter=20,
                         verbose=True)
rs_rf.fit(x_train,y_train)

rs_rf.best_params_

rs_rf.score(x_test,y_test)

model_scores

# GridSearchCV
# Different hyperparameters for LogisticRegression model

log_reg_grid = {"C":np.logspace(-4,4,30),
              "solver":["liblinear"]}

gs_log_reg = GridSearchCV(LogisticRegression(),
                          param_grid=log_reg_grid,
                          cv=5,
                          verbose=True)

gs_log_reg.fit(x_train,y_train)

gs_log_reg.best_params_

gs_log_reg.score(x_test,y_test)

model_scores

# Evaluating the tuned model

y_preds=gs_log_reg.predict(x_test)
y_preds

y_test

# ROC curve
plot_roc_curve(gs_log_reg,x_test,y_test)

# Confusion matrix
print(confusion_matrix(y_test,y_preds))

sns.set(font_scale=1.5)

def plot_conf_mat(y_test,y_preds):
  fig,ax=plt.subplots(figsize=(3,3))
  ax=sns.heatmap(confusion_matrix(y_test,y_preds),
                 annot=True,
                 cbar=False)
  plt.xlabel("True label")
  plt.ylabel("Predicted label")

plot_conf_mat(y_test,y_preds)

# Classification Report
# without cross validation
print(classification_report(y_test,y_preds))

# with cross validation
gs_log_reg.best_params_

clf=LogisticRegression(C= 9.236708571873866,solver="liblinear")

# cross validated accuracy
cv_acc =cross_val_score(clf,x,y,cv=5,scoring="accuracy")
cv_acc

cv_acc = np.mean(cv_acc)
cv_acc

# cross validated precision
cv_precision=cross_val_score(clf,x,y,cv=5,scoring="precision")
cv_precision

cv_precision=np.mean(cv_precision)
cv_precision

# cross validated recall
cv_recall=cross_val_score(clf,x,y,scoring="recall")
cv_recall

cv_recall=np.mean(cv_recall)
cv_recall

# cross validated f1
cv_f1=cross_val_score(clf,x,y,cv=5,scoring="f1")
cv_f1

cv_f1=np.mean(cv_f1)
cv_f1

# Visualize cross-validated metrics

cv_metrics=pd.DataFrame({"Accuracy":cv_acc,
                         "precision":cv_precision,
                         "Recall":cv_recall,
                         "F1":cv_f1},
                          index=[0])
cv_metrics.T.plot.bar(title="cross-validated classification metrics",legend=False)

gs_log_reg.best_params_

# Feature Importance
clf =LogisticRegression(C=9.236708571873866,solver="liblinear")
clf.fit(x_train,y_train)

clf_score=clf.score(x_test,y_test)
clf_score

df.head()

# check co-efficient
clf.coef_

# match coeff to column
feature_dict=dict(zip(df.columns,list(clf.coef_[0])))
feature_dict

# visualize feature importance
feature_df=pd.DataFrame(feature_dict,index=[0])
feature_df.T.plot.bar(title="Feature Importance")
feature_df

pd.crosstab(df["sex"],df["target"])

pd.crosstab(df["slope"],df["target"])

pd.crosstab(df["cp"],df["target"])

df.head()

df.tail()

# sample that predicts 0
sample1 = [60,1,0,150,233,1,1,125,0,2.1,1,2,3]
sample2 =[52,	1,	0,	125,	212,	0,	1	,168,	0	,1.0	,2,	2	,3]

# sample that predicts 1
sample3 =[59	,1,	1,	140,	221,	0,	1,	164,	1	,0.0,	2	,0,	2]
sample4 =[50,	0,	0	,110,	254	,0,	0	,159,	0,	0.0,	2,	0	,2]

# convert the list to array and reshape it to predict
np_array=np.asarray(sample1)
# reshape
reshape=np_array.reshape(-1,1).T
# or use
# reshape=np_array.reshape(1,-1)

# Generate predictions for samples
# predictions = clf.predict(reshape)
# print(predictions)

# prediction with randomized search using random forest
predi =rs_rf.predict(reshape)
print(predi)

# Pickle to export the model
import pickle
filename="heart_disease_prediction.pkl"
pickle.dump(clf,open(filename,"wb"))

# load the exported model to predict
loaded_model=pickle.load(open(filename,"rb"))
predict=loaded_model.predict(reshape)
print(predict)

# Pickle to export the model
import pickle
filename1="heart_disease_prediction_random_forest.pkl"
pickle.dump(rs_rf,open(filename1,"wb"))

loaded_model=pickle.load(open(filename1,"rb"))
predict=loaded_model.predict(reshape)
print(predict)

import joblib
joblib.dump(rs_rf, filename1)

# load the model from disk
loaded_model = joblib.load(filename1)
result = loaded_model.score(X_test, Y_test)
print(result)